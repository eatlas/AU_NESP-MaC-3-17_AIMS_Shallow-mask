import geopandas as gpd
import pandas as pd
import os
import argparse
from geopy.distance import geodesic
import matplotlib.pyplot as plt
import numpy as np
from shapely.geometry import Point, Polygon, MultiPolygon
from shapely.ops import nearest_points
import warnings
from tqdm import tqdm  # For progress indication

warnings.filterwarnings("ignore", category=UserWarning)  # Suppress CRS mismatch warnings

"""
Script: 07c-assess-qaqc-points.py

Description:
This script assesses the positional accuracy of reef boundary data by comparing ground truth points (gtp)
against a matching boundary polygon. The script performs the following steps:

1. **Finding Closest Boundary Edge to Ground Truth Points**:
   - For each point in the ground truth points shapefile (which corresponds to manually adjusted points
     matching the true reef boundary), the script finds the closest point on the nearest polygon
     boundary. This includes points located along the edges between vertices, providing sub-meter
     accuracy.

2. **Calculating Positional Errors**:
   - Calculates the geodetic distance (in meters) between each gtp point and its corresponding
     closest point on the polygon boundary (matching point).

3. **Assigning Regions**:
   - Assigns each point to a region based on a provided regions shapefile. This allows for regional
     analysis of positional errors.

4. **Output**:
   - Saves a CSV file containing gtp coordinates, matching coordinates, IDs, errors, and region
     information to OUTPUT_DIR.
   - Saves a shapefile of the matching points generated during the analysis to
     OUTPUT_DIR for verification and visualization purposes.
   - Generates summary statistics describing the distribution of errors per region, including mean,
     median, standard deviation, percentiles, and the number of points used (`n`).
   - Generates a cumulative distribution function (CDF) plot of the distance errors with an overlaid
     cumulative distribution curve, saved as a PNG file to the output directory.

Arguments:
    --input       Path to the original polygon shapefile to process. This polygon represents the
                  matching boundary against which positional accuracy is assessed.
    --gtp         Path to the ground truth points shapefile. These points are manually adjusted to match
                  the true reef boundary. These are generated by 07b.
    --regions     Path to the study region shapefile used to assign regions to points.

Assess the rough reef masking
python 07c-qaqc-assess-boundary-error.py --input working-data/03-rough-reef-mask_poly/AU_Rough-reef-shallow-mask-with-GBR.shp --gtp in-data/AU_Shallow-mask-boundary_qaqc/AU_Shallow-mask_gtp_B1.shp --regions in-data/AU_NESP-MaC-3-17_Study-boundary/Reef-mapping-study-boundary.shp

Assess the semi-automated mapping:
python 07c-qaqc-assess-boundary-error.py --input out-data/V1/AU_NESP-MaC-3-17_AIMS_Shallow-mask_Medium-Low_V1.shp --gtp in-data/AU_Shallow-mask-boundary_qaqc/AU_Shallow-mask_gtp_B1.shp --regions in-data/AU_NESP-MaC-3-17_Study-boundary/Reef-mapping-study-boundary.shp

Notes:
- The matching points are saved for verification, allowing users to visualize and check the points
  found by the script.
"""
OUTPUT_DIR = "working-data/07-qaqc/"
# Define max_distance in degrees for a polygon to be considered as a match.
# For 3 km, in degrees at equator: 3 km / 111 km per degree ~ 0.027 degrees
MAX_MATCH_DISTANCE_DEG = 0.01

def assign_regions(points_gdf, regions_shp):
    """
    Assign regions to points based on spatial join with region polygons.
    """
    regions_gdf = gpd.read_file(regions_shp)
    points_with_regions = gpd.sjoin(points_gdf, regions_gdf, how='left', predicate='intersects')
    return points_with_regions


def generate_matching_points(polygon_gdf, gtp_gdf):
    """
    Generates matching points by finding the closest point on the nearest polygon boundary
    for each point in gtp_gdf.

    Parameters:
        polygon_gdf (GeoDataFrame): GeoDataFrame containing the polygons.
        gtp_gdf (GeoDataFrame): GeoDataFrame containing the ground truth points.

    Returns:
        match_gdf (GeoDataFrame): GeoDataFrame containing the matching points with IDs matching gtp_gdf.
    """
    # Build spatial index for polygons
    polygon_gdf = polygon_gdf.reset_index(drop=True)
    polygon_sindex = polygon_gdf.sindex  # Spatial index

    # Prepare list to store matching points
    match_points = []

    print(f"Finding closest points for {len(gtp_gdf)} gtp points...")

    

    # Iterate over each gtp point with progress indication
    for idx, row in tqdm(gtp_gdf.iterrows(), total=gtp_gdf.shape[0], desc="Processing points"):
        point = row.geometry  # Shapely Point object

        # Use sindex.nearest to find the nearest polygon index
        indices = polygon_sindex.nearest(
            [point],
            return_all=False,
            max_distance=MAX_MATCH_DISTANCE_DEG,
            return_distance=False,
        )

        if len(indices[1]) == 0:
            # Don't print because it will screw up the progress bar.
            #print(f"No nearest polygon found within {MAX_MATCH_DISTANCE_DEG} degrees for point ID {row['ID']}")
            continue

        # Get the index of the nearest polygon
        nearest_polygon_index = indices[1][0]
        nearest_polygon = polygon_gdf.geometry.iloc[nearest_polygon_index]

        # Get the closest point on the boundary of the nearest polygon
        boundary = nearest_polygon.boundary  # LineString or MultiLineString

        # Use shapely's nearest_points function to find the exact closest point
        nearest_point = nearest_points(point, boundary)[1]

        match_points.append({
            "ID": row["ID"],
            "geometry": nearest_point
        })

    # Create a GeoDataFrame for the matching points
    match_gdf = gpd.GeoDataFrame(match_points, crs=gtp_gdf.crs)
    return match_gdf

def calculate_distances_with_regions(polygon_shp, gtp_file, regions_shp, output_csv, match_points_shp):
    # Load the gtp points shapefile
    gtp_gdf = gpd.read_file(gtp_file)

    # Load the polygon shapefile
    polygon_gdf = gpd.read_file(polygon_shp)

    # Ensure CRS matches
    if polygon_gdf.crs != gtp_gdf.crs:
        polygon_gdf = polygon_gdf.to_crs(gtp_gdf.crs)

    # Generate the matching points
    print("Generating matching points by finding closest points on the nearest polygon boundary...")
    match_gdf = generate_matching_points(polygon_gdf, gtp_gdf)

    # Initialize results list
    results = []

    # Process each gtp point
    print("Calculating distances and updating match points with error values...")
    for idx, gtp_point in tqdm(gtp_gdf.iterrows(), total=gtp_gdf.shape[0], desc="Processing points"):
        gtp_coords = (gtp_point.geometry.y, gtp_point.geometry.x)
        match_point = match_gdf[match_gdf["ID"] == gtp_point["ID"]]

        # Initialize result entry
        result = {
            "ID": gtp_point["ID"],
            "ActLat": gtp_coords[0],
            "ActLon": gtp_coords[1],
            "MatchLat": None,
            "MatchLon": None,
            "Error_m": None,
            "MatchFnd": False
        }

        # If a match is found, calculate the error and update match_gdf
        if not match_point.empty:
            match_coords = (match_point.iloc[0].geometry.y, match_point.iloc[0].geometry.x)
            distance_m = geodesic(gtp_coords, match_coords).meters

            # Update the result and match_gdf
            result.update({
                "MatchLat": match_coords[0],
                "MatchLon": match_coords[1],
                "Error_m": distance_m,
                "MatchFnd": True
            })
            match_gdf.loc[match_gdf["ID"] == gtp_point["ID"], "Error_m"] = distance_m

        # Append the result
        results.append(result)

    # Save the matching points with Error_m to a shapefile
    match_gdf.to_file(match_points_shp)
    print(f"Matching points with errors saved to {match_points_shp}")

    # Create a GeoDataFrame for the results
    results_df = pd.DataFrame(results)
    results_gdf = gpd.GeoDataFrame(
        results_df,
        geometry=gpd.points_from_xy(results_df["ActLon"], results_df["ActLat"]),
        crs=gtp_gdf.crs
    )

    # Assign regions to points
    distances_with_regions = assign_regions(results_gdf, regions_shp)

    # Drop unnecessary columns
    distances_with_regions = distances_with_regions.drop(columns=["geometry", "index_right", "id"], errors="ignore")

    # Save to CSV
    distances_with_regions.to_csv(output_csv, index=False)
    print(f"Results saved to {output_csv}")
    return distances_with_regions



def Dep_calculate_distances_with_regions(polygon_shp, gtp_file, regions_shp, output_csv, match_points_shp):
    
    print("Loading shapefiles...")
    # Load the gtp points shapefile
    gtp_gdf = gpd.read_file(gtp_file)

    # Load the polygon shapefile
    polygon_gdf = gpd.read_file(polygon_shp)

    # Ensure CRS matches
    if polygon_gdf.crs != gtp_gdf.crs:
        polygon_gdf = polygon_gdf.to_crs(gtp_gdf.crs)

    # Generate the matching points
    print("Generating matching points by finding closest points on the nearest polygon boundary...")
    match_gdf = generate_matching_points(polygon_gdf, gtp_gdf)

    # Save the matching points to a shapefile
    match_gdf.to_file(match_points_shp)
    print(f"Matching points saved to {match_points_shp}")

    # Filter dataframes to matching IDs
    gtp_gdf = gtp_gdf.reset_index(drop=True)

    # Calculate distances and include unmatched points
    results = []
    print("Calculating distances between gtp points and matching points...")
    for idx, gtp_point in tqdm(gtp_gdf.iterrows(), total=gtp_gdf.shape[0], desc="Calculating distances"):
        gtp_coords = (gtp_point.geometry.y, gtp_point.geometry.x)
        match_point = match_gdf[match_gdf["ID"] == gtp_point["ID"]]
        if not match_point.empty:
            match_coords = (match_point.iloc[0].geometry.y, match_point.iloc[0].geometry.x)
            distance_m = geodesic(gtp_coords, match_coords).meters
            results.append({
                "ID": gtp_point["ID"],
                "ActLat": gtp_coords[0],
                "ActLon": gtp_coords[1],
                "MatchLat": match_coords[0],
                "MatchLon": match_coords[1],
                "Error_m": distance_m,
                "MatchFnd": True
            })
        else:
            results.append({
                "ID": gtp_point["ID"],
                "ActLat": gtp_coords[0],
                "ActLon": gtp_coords[1],
                "MatchLat": None,
                "MatchLon": None,
                "Error_m": None,
                "MatchFnd": False
            })

    # Create a DataFrame for the results
    results_df = pd.DataFrame(results)

    # Convert to GeoDataFrame
    results_gdf = gpd.GeoDataFrame(
        results_df,
        geometry=gpd.points_from_xy(results_df["ActLon"], results_df["ActLat"]),
        crs=gtp_gdf.crs
    )

    # Assign regions to points
    distances_with_regions = assign_regions(results_gdf, regions_shp)

    # Drop unnecessary columns
    distances_with_regions = distances_with_regions.drop(columns=["geometry", "index_right", "id"], errors="ignore")

    # Save to CSV
    distances_with_regions.to_csv(output_csv, index=False)
    print(f"Results saved to {output_csv}")
    return distances_with_regions
    


def output_error_statistics_by_region(errors_gdf):
    """
    Compute and print error statistics per region, including unmatched locations.
    """
    # Combine NorthernAU and RemoteReefs into a single category
    errors_gdf["RegionCategory"] = errors_gdf["RegionName"].replace({"RemoteReefs": "NorthernAU"})

    # Group by RegionCategory
    for region, group in errors_gdf.groupby("RegionCategory"):
        errors = group["Error_m"]
        match_found = group["MatchFnd"]

        # Calculate statistics for matched points only
        matched_errors = errors[match_found]
        n_matched = len(matched_errors)
        n_unmatched = len(match_found) - n_matched  # Total points minus matched points

        if n_matched > 0:
            mean_error = matched_errors.mean()
            std_dev_error = matched_errors.std()
            median_error = matched_errors.median()
            perc_90 = np.percentile(matched_errors.dropna(), 90)
            perc_95 = np.percentile(matched_errors.dropna(), 95)
            max_error = matched_errors.max()
        else:
            # Handle cases with no matches
            mean_error = std_dev_error = median_error = perc_90 = perc_95 = max_error = None

        print(f"{region} Error Statistics:")
        print(f"  Total Number of Points: {len(match_found)}")
        print(f"  Matched Points (n): {n_matched}")
        print(f"  Unmatched Points (n): {n_unmatched}")
        if n_matched > 0:
            print(f"  Mean Error (m): {mean_error:.2f}")
            print(f"  Std Dev Error (m): {std_dev_error:.2f}")
            print(f"  Median Error (m): {median_error:.2f}")
            print(f"  90th Percentile Error (m): {perc_90:.2f}")
            print(f"  95th Percentile Error (m): {perc_95:.2f}")
            print(f"  Maximum Error (m): {max_error:.2f}")
        else:
            print("  No matches found in this region.")
        print()


def generate_cdf_by_regions(errors_gdf, output_cdf, shapefile_name):
    """
    Generate a cumulative distribution plot for each region category.
    """
    # Combine NorthernAU and RemoteReefs into a single category
    errors_gdf["RegionCategory"] = errors_gdf["RegionName"].replace({"RemoteReefs": "NorthernAU"})

    # Increase font sizes by 20%
    title_fontsize = 14
    label_fontsize = 12
    tick_fontsize = 12
    legend_fontsize = 12

    # Calculate cumulative distributions for each region
    plt.figure(figsize=(8, 5), dpi=200)
    for region, group in errors_gdf.groupby("RegionCategory"):
        errors = group["Error_m"].clip(lower=2.5)  # Clip errors to 2.5 meters minimum
        sorted_errors = np.sort(errors.to_numpy())
        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100  # Percentage

        plt.plot(sorted_errors, cumulative, linewidth=2, label=f"{region} n={len(group)}")

    # Configure the plot
    plt.xscale("log")
    plt.xlabel("Error Distance (m)", fontsize=label_fontsize)
    plt.ylabel("Cumulative Percentage (%)", fontsize=label_fontsize)
    plt.title(f"Digitisation boundary error by region\n{shapefile_name}", fontsize=title_fontsize)
    plt.yticks(np.arange(0, 101, 10), fontsize=tick_fontsize)  # Major ticks every 10
    plt.gca().yaxis.set_minor_locator(plt.MultipleLocator(5))  # Minor ticks every 5
    plt.ylim(0, 100)
    plt.xticks(fontsize=tick_fontsize)
    plt.yticks(fontsize=tick_fontsize)
    plt.grid(True, which="both", ls="--", linewidth=0.5)
    plt.legend(loc="lower right", fontsize=legend_fontsize)
    plt.tight_layout()

    # Save the plot
    plt.savefig(output_cdf)
    plt.close()
    print(f"Cumulative distribution plot saved to {output_cdf}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Assess QAQC accuracy.")
    parser.add_argument(
        "--input",
        default="working-data/03-rough-reef-mask_poly/AU_Rough-reef-shallow-mask-with-GBR.shp",
        help="Path to the polygon shapefile assess against the gtp points shapefile.",
    )
    parser.add_argument(
        "--gtp",
        default="in-data/AU_Shallow-mask-boundary_qaqc/AU_Shallow-mask_gtp_B1.shp",
        help="Path to the ground truth points shapefile.",
    )
    parser.add_argument(
        "--regions",
        default="in-data/AU_NESP-MaC-3-17_Study-boundary/Reef-mapping-study-boundary.shp",
        help="Path to the study region shapefile. This is used to subdivide the results.",
    )
    args = parser.parse_args()

    # Define paths
    base_filename = os.path.splitext(os.path.basename(args.input))[0]
    gtp_file = args.gtp
    
    output_csv = f"{OUTPUT_DIR}/{base_filename}_pos-err.csv"
    output_cdf = f"{OUTPUT_DIR}/{base_filename}_error_cdf.png"
    match_points_shp = f"{OUTPUT_DIR}/{base_filename}_match_pnts.shp"

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Calculate and save results
    distances_with_regions = calculate_distances_with_regions(
        args.input, gtp_file, args.regions, output_csv, match_points_shp
    )

    if distances_with_regions is not None:
        output_error_statistics_by_region(distances_with_regions)
        generate_cdf_by_regions(distances_with_regions, output_cdf, base_filename)
